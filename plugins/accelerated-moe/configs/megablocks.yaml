training:

  # mixture-of-experts configurations
  moe: 

    # expert-parallel for MoE
    megablocks:

      dummy: 1

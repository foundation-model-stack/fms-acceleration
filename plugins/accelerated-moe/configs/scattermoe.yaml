training:

  # mixture-of-experts configurations
  moe:

    # expert-parallel for MoE
    scattermoe:


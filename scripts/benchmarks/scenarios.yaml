# This file holds a list of scenarios to may be run.
# - to limit to a number of scenarios, use the --run-only-scenarios flag.
# - Each scenario will be run against a particular acceleration framework
#   config, if the framework_config: key is specified.
#   * a particular framework configuration
# - the arguments tag will hold arguments to be passed to sft_trainer
#   * the arguments are singular except for model_name_or_path which can handle
#     multiple arguments.
# - So anything that is critical for the scenario MUST be specified here 
#   and not in the defaults, e.g. fp16
scenarios:
    -   name: full-finetuning
        arguments:
            learning_rate: 2e-5
            model_name_or_path: 
                - 'mistralai/Mistral-7B-v0.1'
                - 'mistralai/Mixtral-8x7B-Instruct-v0.1'
                - 'NousResearch/Llama-2-70b-hf'
            torch_dtype: float16

    -   name: standard-peft
        arguments:
            learning_rate: 2e-4
            torch_dtype: float16
            peft_method: lora
            r: 16
            lora_alpha: 16
            lora_dropout: 0.0
            target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
            model_name_or_path: 
                - 'mistralai/Mistral-7B-v0.1'
                - 'mistralai/Mixtral-8x7B-Instruct-v0.1'
                - 'NousResearch/Llama-2-70b-hf'

    -   name: baseline-peft-bnb
        framework_config: 
            - baseline-peft-bnb
        arguments:
            fp16: True
            learning_rate: 2e-4
            torch_dtype: float16
            peft_method: lora
            r: 16
            lora_alpha: 16
            lora_dropout: 0.0
            target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
            model_name_or_path: 
                - 'mistralai/Mistral-7B-v0.1'
                - 'mistralai/Mixtral-8x7B-Instruct-v0.1'
                - 'NousResearch/Llama-2-70b-hf'

    -   name: accelerated-peft-bnb
        framework_config: 
            - accelerated-peft-bnb
            - accelerated-peft-autogptq-foak
        arguments:
            fp16: True
            learning_rate: 2e-4
            torch_dtype: float16
            peft_method: lora
            r: 16
            lora_alpha: 16
            lora_dropout: 0.0
            target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
            model_name_or_path: 
                - 'mistralai/Mistral-7B-v0.1'
                - 'mistralai/Mixtral-8x7B-Instruct-v0.1'
                - 'NousResearch/Llama-2-70b-hf'

    -   name: accelerated-peft-gptq
        framework_config: 
            - accelerated-peft-autogptq
            - accelerated-peft-autogptq-foak
        arguments:
            learning_rate: 2e-4
            fp16: True
            torch_dtype: float16
            peft_method: lora
            r: 16
            lora_alpha: 16
            lora_dropout: 0.0
            target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
            model_name_or_path: 
                - 'TheBloke/Mistral-7B-v0.1-GPTQ'
                - 'TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ'
                - 'TheBloke/Llama-2-70B-GPTQ'